{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661506ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade lamini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617d48a-7e79-484b-a1ea-6e3c5faba8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade lamini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84af40d-a4a2-4a95-81f7-881f9b255b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0443223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45049715-65a3-4488-8a5b-ad42635924df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import jsonlines\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99aaa-f2fb-4818-aae6-4ad479d317c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama import BasicModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d92d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Two Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27440cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for j in top_m:\n",
    "    if not j[\"input\"]:\n",
    "        processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
    "    else:\n",
    "        processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
    "\n",
    "    processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83836e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fe256",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save to jsonl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff050236",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Try smaller models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56951c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = finetuning_dataset[\"test\"][0]\n",
    "print(test_sample)\n",
    "\n",
    "print(inference(test_sample[\"question\"], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compare to finetuned small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/ksm26/Finetuning-Large-Language-Models/blob/main/03_Instruction_tuning.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
